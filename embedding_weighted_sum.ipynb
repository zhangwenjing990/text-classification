{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.random.manual_seed(2333)\n",
    "\n",
    "device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 200\n",
    "GRAD_CLIP = 2\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.5\n",
    "LOG_FILE = 'classification_21.log'\n",
    "model_path = './best_21'\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'[^a-z]+',r' ',s)\n",
    "    return s\n",
    "\n",
    "def data_preprocess(dir_path, file_name):\n",
    "    word2index = {}\n",
    "    kind_of_words = 2\n",
    "    max_sequence_len = 0\n",
    "    word_count = {}\n",
    "\n",
    "    with open(dir_path + '/' + file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        text = line.split('\\t')[0]\n",
    "        normalized_text = normalize_string(text)\n",
    "        sequence_len = len(normalized_text.split())\n",
    "        if sequence_len > max_sequence_len:\n",
    "            max_sequence_len =sequence_len\n",
    "        for word in normalized_text.split():\n",
    "            if word not in word2index:\n",
    "                word2index[word] = kind_of_words\n",
    "                kind_of_words += 1\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "\n",
    "    return word2index, max_sequence_len, kind_of_words, word_count\n",
    "\n",
    "word2index, max_sequence_len, kind_of_words, word_count = data_preprocess('text_classification_data', 'senti.train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dir_path, file_name, max_sequence_len=50):\n",
    "        with open(dir_path + '/' + file_name, 'r', encoding='utf-8') as f:\n",
    "            self.lines = f.readlines()\n",
    "\n",
    "        self.length = len(self.lines)\n",
    "        self.pad = 0\n",
    "        self.unknown = 1\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "\n",
    "    def __len__(self):\n",
    "        length = 0\n",
    "        for line in self.lines:\n",
    "            text = line.strip().split('\\t')[0]\n",
    "            normalized_text = normalize_string(text)\n",
    "            if len(normalized_text.split()) !=0:\n",
    "                length +=1\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        line = self.lines[item]\n",
    "        line = line.strip().split('\\t')\n",
    "        text, label = line[0], line[1]\n",
    "        normalized_text = normalize_string(text)\n",
    "        text_token = [word2index[word] if word in word2index else self.unknown for word in normalized_text.split()]\n",
    "        if self.max_sequence_len > len(text_token):\n",
    "            padded_text_token = text_token + [self.pad]*(self.max_sequence_len-len(text_token))\n",
    "        else:\n",
    "            padded_text_token = text_token[:self.max_sequence_len]\n",
    "        return torch.tensor(padded_text_token), int(label)\n",
    "\n",
    "train_dataset = CustomDataset('text_classification_data', 'senti.cleaned_train.tsv', max_sequence_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataset = CustomDataset('text_classification_data', 'senti.cleaned_dev.tsv', max_sequence_len)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataset = CustomDataset('text_classification_data', 'senti.cleaned_test.tsv', max_sequence_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义文本分类模型\n",
    "class CustomTextClassification(nn.Module):\n",
    "    def __init__(self, embedding_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(embedding_size, embedding_dim, padding_idx=0)\n",
    "        self.attention = nn.Linear(embedding_dim, 1, bias=False)\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.epsilon = torch.tensor(1e-8,device=device)\n",
    "        self.init_weights()\n",
    "        # self.epsilon = torch.tensor(1e-8, device=device)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.attention.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        padding = (input_tensor == torch.tensor(0, device=device)).to(dtype=torch.float32) # [batch_size]\n",
    "        masked = -padding*10000\n",
    "        embed_out = self.dropout(self.embed(input_tensor)) # [batch_size, padded_sequence_len, embedding_dim]\n",
    "        attention_score = self.attention(embed_out).squeeze(-1) # [batch_size, padded_sequence_len]\n",
    "        attention_norm = torch.div(attention_score, (embed_out.norm(dim=-1) + self.epsilon))\n",
    "        attention_weight = torch.softmax((attention_norm + masked), dim=-1) # [batch_size, padded_sequence_len]\n",
    "        hidden_attention = torch.mul(embed_out, attention_weight.unsqueeze(-1)).sum(1)\n",
    "        decode = self.fc(hidden_attention)\n",
    "        output = torch.sigmoid(decode) # [batch_size, 1]\n",
    "\n",
    "        return output.squeeze(1)\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = torch.tensor(1e-8)\n",
    "\n",
    "    def forward(self, output, labels):\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "        loss_1 = -labels*torch.log(output + self.epsilon)\n",
    "        loss_0 = -(torch.tensor(1.) - labels)*torch.log(torch.tensor(1.) - output + self.epsilon)\n",
    "        loss = torch.mean(loss_0 + loss_1)\n",
    "        return loss\n",
    "\n",
    "model = CustomTextClassification(embedding_size=kind_of_words, embedding_dim=EMBEDDING_DIM).to(device)\n",
    "criterion = CustomLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        total_accurate = 0.\n",
    "        total_count = 0.\n",
    "        for text, label in data_loader:\n",
    "            text, label = text.to(device), label.to(device)\n",
    "            text_count = torch.numel(label)\n",
    "\n",
    "            output = model(text)\n",
    "            loss = criterion(output, label)\n",
    "            out_label = (output > 0.5).to(dtype=label.dtype)\n",
    "            accurate = (out_label == label).sum()\n",
    "\n",
    "            total_accurate += accurate.item()\n",
    "            total_loss += loss.item() * text_count\n",
    "            total_count += text_count\n",
    "\n",
    "        return total_loss / total_count, total_accurate / total_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "best_valid_accuracy = 0.\n",
    "for epoch in range(EPOCHS):\n",
    "    total_train_loss = 0.\n",
    "    total_train_count = 0.\n",
    "    for iter, (text, label) in enumerate(train_loader):\n",
    "        # 数据准备\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        # 模型准备\n",
    "        model.train() # 训练模式，对应与评估模式，影响模型的dropout,BatchNorm等\n",
    "        model.zero_grad() # 梯度清零\n",
    "        #前向计算\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        #反向传播\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step() # 参数更新\n",
    "        #结果输出\n",
    "        text_count = torch.numel(label)\n",
    "        total_train_loss += loss.item() * text_count\n",
    "        total_train_count += text_count\n",
    "\n",
    "    val_loss, val_accuracy = evaluate(model, valid_loader)\n",
    "    print('epoch:{}'.format(epoch))\n",
    "    print('\\ttrain_batch_loss  :{:.10f}'.format(total_train_loss / total_train_count))\n",
    "    print('\\tval_loss          :{:.10f} | val_accuracy:{}'.format(val_loss, val_accuracy))\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write('epoch:{},iter:{},train_batch_loss:{},val_loss:{},val_accuracy:{}\\n'.format(epoch, iter, loss.item(),\n",
    "                                                                                            val_loss, val_accuracy))\n",
    "        if val_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid_loss: 0.4318920254 | Valid_acc: 0.8176605504587156\n",
      "Test_loss:  0.4169149343 | Test_acc:  0.8066996155958265\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "valid_loss, valid_acc = evaluate(model, valid_loader)\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print('Valid_loss: {:.10f} | Valid_acc: {}'.format(valid_loss,valid_acc))\n",
    "print('Test_loss:  {:.10f} | Test_acc:  {}'.format(test_loss,test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity最高的15个单词：\n",
      " ['not', 'never', 'unconvincing', 'worst', 'less', 'negotiate', 'unfortunately', 'none', 'mess', 'failed', 'missed', 'unpleasant', 'vivi', 'fails', 'cletis']\n",
      "cosine similarity最低的15个单词：\n",
      " ['trivializing', 'outtakes', 'literarily', 'this', 'film', 'cinderella', 'trot', 'an', 'as', 'with', 'spiffy', 'in', 'has', 'that', 's']\n"
     ]
    }
   ],
   "source": [
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "embedding = model.embed.weight.data\n",
    "u = model.attention.weight.data\n",
    "cosin_similarity = (embedding*u).sum(-1) / (embedding.norm(dim=-1) + torch.tensor(1e-8))\n",
    "\n",
    "_,sorted_index = cosin_similarity.sort(descending=True)\n",
    "max_similarity = [index2word[index] for index in sorted_index[:15].tolist()]\n",
    "min_similarity = [index2word[index] for index in sorted_index[-16:-1].tolist()] \n",
    "print('cosine similarity最高的15个单词：\\n',max_similarity)\n",
    "print('cosine similarity最低的15个单词：\\n',min_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算权重和标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
    "most_word = {item[0]:item[1] for item in sorted_word if item[1] > 100}\n",
    "with open('text_classification_data/senti.cleaned_train.tsv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attention = {}\n",
    "\n",
    "def calculate_attention(text, attention):\n",
    "    text_tokenize = [word2index[word] for word in text.split()]\n",
    "    word_embedding = embedding[torch.tensor(text_tokenize)]\n",
    "    cosin_similarity = (word_embedding*u).sum(-1) / word_embedding.norm(dim=-1) /u.norm()\n",
    "    attention_weight = torch.softmax(cosin_similarity,dim=-1).tolist()\n",
    "    for index, word in enumerate(text.split()):\n",
    "        if word in most_word:\n",
    "            if word not in word_attention:\n",
    "                word_attention[word] = []\n",
    "            word_attention[word].append(attention_weight[index])\n",
    "    return word_attention\n",
    "\n",
    "for line in lines:\n",
    "    text = line.split('\\t')[0]\n",
    "    for word in text.split():\n",
    "        if word in most_word:\n",
    "            word_attention = calculate_attention(text, word_attention)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_standard = {word:torch.tensor(word_attention[word]).std().item() for word in word_attention}\n",
    "sorted_word_standard = sorted(word_standard.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_std = sorted_word_standard[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_std_dic = {word:std for word,std in max_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stupid         :0.2192760855\n",
      "pretentious    :0.2149189413\n",
      "worse          :0.2127662003\n",
      "inventive      :0.2123491913\n",
      "tedious        :0.2096301317\n",
      "watchable      :0.2088883817\n",
      "endearing      :0.2067171931\n",
      "mess           :0.1989077479\n",
      "painful        :0.1986908019\n",
      "dumb           :0.1956243962\n",
      "masterpiece    :0.1948784143\n",
      "gorgeous       :0.1944991797\n",
      "warm           :0.1936010122\n",
      "slow           :0.1935067922\n",
      "terrific       :0.1934913546\n",
      "unfunny        :0.1911743283\n",
      "ambitious      :0.1892865747\n",
      "convincing     :0.1859398335\n",
      "awful          :0.1858885437\n",
      "provocative    :0.1850747913\n",
      "hip            :0.1849068403\n",
      "epic           :0.1830532402\n",
      "wonderful      :0.1826217473\n",
      "beautifully    :0.1818212718\n",
      "appealing      :0.1811695248\n",
      "flat           :0.1801966578\n",
      "intelligent    :0.1800693870\n",
      "bland          :0.1797271669\n",
      "delightful     :0.1789504588\n",
      "shallow        :0.1779978573\n"
     ]
    }
   ],
   "source": [
    "for word, standard in max_std_dic.items():\n",
    "    print('{:15s}:{:.10f}'.format(word,standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_class_count={}\n",
    "for line in lines:\n",
    "    text, label = line.strip().split('\\t')\n",
    "    for word in text.split():\n",
    "        if word in max_std_dic:\n",
    "            if word not in word_class_count:\n",
    "                word_class_count[word] = []\n",
    "            word_class_count[word].append(int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计这些单词所在的句子类别为1的概率\n",
      "dumb           :0.198\n",
      "unfunny        :0.000\n",
      "worse          :0.008\n",
      "gorgeous       :0.930\n",
      "beautifully    :0.969\n",
      "terrific       :0.981\n",
      "masterpiece    :0.782\n",
      "epic           :0.806\n",
      "flat           :0.103\n",
      "awful          :0.142\n",
      "mess           :0.027\n",
      "watchable      :0.772\n",
      "delightful     :0.898\n",
      "shallow        :0.018\n",
      "tedious        :0.048\n",
      "warm           :0.909\n",
      "intelligent    :0.872\n",
      "endearing      :0.792\n",
      "ambitious      :0.725\n",
      "wonderful      :0.954\n",
      "slow           :0.231\n",
      "appealing      :0.927\n",
      "stupid         :0.014\n",
      "hip            :0.392\n",
      "convincing     :0.791\n",
      "painful        :0.270\n",
      "provocative    :0.806\n",
      "bland          :0.110\n",
      "pretentious    :0.017\n",
      "inventive      :0.973\n"
     ]
    }
   ],
   "source": [
    "print('统计这些单词所在的句子类别为1的概率')\n",
    "for word in word_class_count:\n",
    "    print('{:15s}:{:.3f}'.format(word, sum(word_class_count[word]) / len(word_class_count[word])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
