{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于有些文本只有数字，我认为对不同的数字分类没有意义，所以我提前对数据进行了清洗，生成3个新的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出标签1的概率，自定义loss计算损失\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.random.manual_seed(2333)\n",
    "\n",
    "device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 200\n",
    "GRAD_CLIP = 2\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.5\n",
    "LOG_FILE = 'classification_13.log'\n",
    "model_path = './best_13'\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'[^a-z]+',r' ',s)\n",
    "    return s\n",
    "\n",
    "def data_preprocess(dir_path, file_name):\n",
    "    word2index = {}\n",
    "    kind_of_words = 2\n",
    "    max_sequence_len = 0\n",
    "\n",
    "    with open(dir_path + '/' + file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        text = line.split('\\t')[0]\n",
    "        normalized_text = normalize_string(text)\n",
    "        sequence_len = len(normalized_text.split())\n",
    "        if sequence_len > max_sequence_len:\n",
    "            max_sequence_len =sequence_len\n",
    "        for word in normalized_text.split():\n",
    "            if word not in word2index:\n",
    "                word2index[word] = kind_of_words\n",
    "                kind_of_words += 1\n",
    "\n",
    "    return word2index, max_sequence_len, kind_of_words\n",
    "\n",
    "word2index, max_sequence_len, kind_of_words = data_preprocess('text_classification_data', 'senti.train.tsv')\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dir_path, file_name, max_sequence_len=50):\n",
    "        with open(dir_path + '/' + file_name, 'r', encoding='utf-8') as f:\n",
    "            self.lines = f.readlines()\n",
    "\n",
    "        self.length = len(self.lines)\n",
    "        self.pad = 0\n",
    "        self.unknown = 1\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "\n",
    "    def __len__(self):\n",
    "        length = 0\n",
    "        for line in self.lines:\n",
    "            text = line.strip().split('\\t')[0]\n",
    "            normalized_text = normalize_string(text)\n",
    "            if len(normalized_text.split()) !=0:\n",
    "                length +=1\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        line = self.lines[item]\n",
    "        line = line.strip().split('\\t')\n",
    "        text, label = line[0], line[1]\n",
    "        normalized_text = normalize_string(text)\n",
    "        text_token = [word2index[word] if word in word2index else self.unknown for word in normalized_text.split()]\n",
    "        if self.max_sequence_len > len(text_token):\n",
    "            padded_text_token = text_token + [self.pad]*(self.max_sequence_len-len(text_token))\n",
    "        else:\n",
    "            padded_text_token = text_token[:self.max_sequence_len]\n",
    "        return torch.tensor(padded_text_token), int(label)\n",
    "\n",
    "train_dataset = CustomDataset('text_classification_data', 'senti.cleaned_train.tsv', max_sequence_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "valid_dataset = CustomDataset('text_classification_data', 'senti.cleaned_dev.tsv', max_sequence_len)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataset = CustomDataset('text_classification_data', 'senti.cleaned_test.tsv', max_sequence_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextClassification(nn.Module):\n",
    "    def __init__(self, embedding_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(embedding_size, embedding_dim, padding_idx=0)\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.epsilon = torch.tensor(1e-8, device=device)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        sequence_len = (input_tensor != torch.tensor(0, device=device)).sum(-1,keepdim=True) # [batch_size]\n",
    "        embed_out = self.dropout(self.embed(input_tensor)) # [batch_size, padded_sequence_len, embedding_dim]\n",
    "        hidden_average = torch.div(embed_out.sum(1), (sequence_len.to(dtype=embed_out.dtype) + self.epsilon))# [batch_size, embedding_dim]\n",
    "        decode = self.fc(hidden_average)\n",
    "        output = torch.sigmoid(decode) # [batch_size, 1]\n",
    "\n",
    "\n",
    "        return output.squeeze(1)\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = torch.tensor(1e-8)\n",
    "\n",
    "    def forward(self, output, labels):\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "        loss_1 = -labels*torch.log(output + self.epsilon)\n",
    "        loss_0 = -(torch.tensor(1.) - labels)*torch.log(torch.tensor(1.) - output + self.epsilon)\n",
    "        loss = torch.mean(loss_0 + loss_1)\n",
    "        return loss\n",
    "\n",
    "model = CustomTextClassification(embedding_size=kind_of_words, embedding_dim=EMBEDDING_DIM).to(device)\n",
    "criterion = CustomLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        total_accurate = 0.\n",
    "        total_count = 0.\n",
    "        for text, label in data_loader:\n",
    "            text, label = text.to(device), label.to(device)\n",
    "            text_count = torch.numel(label)\n",
    "\n",
    "            output = model(text)\n",
    "            loss = criterion(output, label)\n",
    "            out_label = (output > 0.5).to(dtype=label.dtype)\n",
    "            accurate = (out_label == label).sum()\n",
    "\n",
    "            total_accurate += accurate.item()\n",
    "            total_loss += loss.item() * text_count\n",
    "            total_count += text_count\n",
    "\n",
    "        return total_loss / total_count, total_accurate / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "best_valid_accuracy = 0.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_train_loss = 0.\n",
    "    total_train_count = 0.\n",
    "    for iter, (text, label) in enumerate(train_loader):\n",
    "        # 数据准备\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        # 模型准备\n",
    "        model.train() # 训练模式，对应与评估模式，影响模型的dropout,BatchNorm等\n",
    "        model.zero_grad() # 梯度清零\n",
    "        #前向计算\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        #反向传播\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step() # 参数更新\n",
    "        #结果输出\n",
    "        text_count = torch.numel(label)\n",
    "        total_train_loss += loss.item() * text_count\n",
    "        total_train_count += text_count\n",
    "    # 每个epoch计算验证集准确率\n",
    "    val_loss, val_accuracy = evaluate(model, valid_loader)\n",
    "    print('epoch:{}'.format(epoch))\n",
    "    print('\\ttrain_batch_loss  :{:.10f}'.format(total_train_loss / total_train_count))\n",
    "    print('\\tval_loss          :{:.10f} | val_accuracy:{}'.format(val_loss, val_accuracy))\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write('epoch:{},iter:{},train_batch_loss:{},val_loss:{},val_accuracy:{}\\n'.format(epoch, iter, loss.item(),\n",
    "                                                                                            val_loss, val_accuracy))\n",
    "        if val_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid_loss: 0.4123196883 | Valid_acc: 0.8302752293577982\n",
      "Test_loss:  0.4097559166 | Test_acc:  0.8110928061504667\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "valid_loss, valid_acc = evaluate(model, valid_loader)\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print('Valid_loss: {:.10f} | Valid_acc: {}'.format(valid_loss,valid_acc))\n",
    "print('Test_loss:  {:.10f} | Test_acc:  {}'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算L2-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm最大的15个单词：\n",
      " ['worst', 'lacks', 'lacking', 'hilarious', 'waste', 'mess', 'poor', 'stupid', 'powerful', 'terrific', 'pointless', 'neither', 'lack', 'beautifully', 'nice']\n",
      "norm最小的15个单词：\n",
      " ['film', 'an', 'as', 'that', 'its', 'it', 'in', 'movie', 'is', 'to', 's', 'of', 'a', 'and', 'the']\n"
     ]
    }
   ],
   "source": [
    "index2word = {v:k for k,v in word2index.items()}\n",
    "\n",
    "embedding_norm = model.embed.weight.data.norm(dim=-1)\n",
    "\n",
    "_,sorted_index = embedding_norm.sort(descending=True)\n",
    "max_norm = [index2word[index] for index in sorted_index[:15].tolist()]\n",
    "min_norm = [index2word[index] for index in sorted_index[-16:-1].tolist()] \n",
    "print('norm最大的15个单词：\\n',max_norm)\n",
    "print('norm最小的15个单词：\\n',min_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
